# -*- coding: utf-8 -*-
"""henry_unet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SYkhmEVvUqagvh-zYroytFED6o0mKwe_

# **Installation**
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install keras>=2.2.0
!pip install tensorflow
!pip install keras-applications >= 1.0.7, <=1.0.8
!pip install image-classifiers==1.0.0
!pip install efficientnet==1.0.0
!pip install -U albumentations>=0.3.0 --user 
#!pip install -U --pre segmentation-models --user
!pip install -U segmentation-models #Stable package
!pip install -U --pre segmentation-models --user
#!pip install -U --pre segmentation-models #Latest package
#!pip install git+https://github.com/qubvel/segmentation_models #Source latest version

import os
os.environ['SM_FRAMEWORK'] = 'tf.keras'
import segmentation_models as sm

print('sm.version=' + sm.__version__)
sm.set_framework('tf.keras')
from segmentation_models import Unet
from segmentation_models import FPN
from segmentation_models import get_preprocessing
from segmentation_models.losses import bce_jaccard_loss
from segmentation_models.metrics import iou_score
import numpy as np
import matplotlib.pyplot as plt
from keras.optimizer_v1 import Adam

import os
import numpy as np
from skimage.io import imread
from skimage.transform import resize
import random as rd
data_dir = '/content/drive/MyDrive/Modelos/Data'
#/content/drive/MyDrive/Modelos/Data/Imagen/Imagenes

#read train
def read_dataset(p,alfa=0.8,name_images = "Imagen/Imagenes",name_etiquetas = "Mask/ImagenesMask"):
  path_ima = [os.path.join(data_dir,name_images, filename) for filename in os.listdir(os.path.join(data_dir,name_images))]
  path_etiquetas = [os.path.join(data_dir,name_etiquetas, filename) for filename in os.listdir(os.path.join(data_dir,name_etiquetas))]
  images = [] ; etiquetas = []
  path_ima.sort()
  path_etiquetas.sort()
  #p = len(path_ima)
  cant = int(p * alfa)-1
  for i in range(p):
    if path_ima[i].replace(data_dir+"/Imagenes","").replace(".png","") != path_etiquetas[i].replace(data_dir+"/ImagenesMask","").replace("_gt_bbox.jpg",""):
      img = resize(imread(path_ima[i])/255,(256,256),mode='constant', preserve_range=True).astype('float32')
      images.append(img)
      etiq = resize(imread(path_etiquetas[i])/255,(256,256),mode='constant', preserve_range=True).astype('float32')
      etiquetas.append(etiq)
  x_train = np.stack(images[:cant], axis= 0)
  y_train = np.stack(etiquetas[:cant], axis= 0)
  x_val = np.stack(images[cant:], axis= 0)
  y_val = np.stack(etiquetas[cant:], axis= 0)
  return [x_train, y_train , x_val , y_val ]

def load_data(root_dir):
    data = []
    for stage in ['train', 'val']:
        for content in ['images', 'masks']:
            # construct path to each image
            directory = os.path.join(root_dir, stage, content)
            #fps = [ for filename in os.listdir(directory)]
            # read images
            images = [imread(os.path.join(directory, filename)) for filename in os.listdir(directory)]
            resized_images = [resize(image,(256, 256)) for image in images]
            # stack to one np.array 
            np_images = np.stack(resized_images, axis=0)
            print(np_images.shape)
            #print(np_images.size())
           # print(len(resized_images))
            data.append(np_images/255)
    return data

BACKBONE = 'vgg16'
preprocess_input = get_preprocessing(BACKBONE)
from tensorflow.keras.optimizers import Adam
# load your data
#x_train, y_train, x_val, y_val = load_data(data_dir)
x_train, y_train, x_val, y_val = read_dataset(p=200)

# preprocess input
#x_train = preprocess_input(x_train)
#x_val = preprocess_input(x_val)

LR = 0.001
optim = Adam(LR)
# define model
model = FPN(BACKBONE, classes=1,activation='sigmoid',input_shape=(256,256,3),encoder_weights="imagenet",)
model.compile(optim, loss=bce_jaccard_loss, metrics=[sm.metrics.iou_score])
#model.compile(optimizer='sgd', loss=loss, metrics=[dice_coefficient])

# fit model
y_val =  y_val[:,:,:,0:1]
history = model.fit(
    x=x_train,
    y=y_train[:,:,:,0:1],
    batch_size=10,
    epochs=60,
    validation_data=(x_val,y_val)
)

# Plot training & validation iou_score values
import matplotlib.pyplot as plt 

plt.figure(figsize=(30, 5))
plt.subplot(121)
plt.plot(history.history['iou_score'])
plt.plot(history.history['val_iou_score'])
plt.title('Model iou_score')
plt.ylabel('iou_score')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')

# Plot training & validation loss values
plt.subplot(122)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.savefig(data_dir+"/"+"SegundaFigura.png")
plt.show()

def visualize(**images):
    """PLot images in one row."""
    n = len(images)
    plt.figure(figsize=(16, 5))
    for i, (name, image) in enumerate(images.items()):
        plt.subplot(1, n, i + 1)
        plt.xticks([])
        plt.yticks([])
        plt.title(' '.join(name.split('_')).title())
        plt.imshow(image)
    plt.show()

imag_str = "/content/drive/MyDrive/Modelos/Data/Imagen/Imagenes/20160331_NTU_00001.png"
path_mask =  "/content/drive/MyDrive/Modelos/Data/Mask/ImagenesMask/20160331_NTU_00001_gt_bbox.jpg"
image = resize(imread(imag_str)/255,(256,256)).astype('float32')
mask = resize(imread(path_mask)/255,(256,256)).astype('float32')
image = np.expand_dims(image, axis=0)
pr_mask = model.predict(image).round()
visualize(
        image=image.squeeze(),
        gt_mask=mask[..., 0].squeeze(),
        pr_mask=pr_mask[..., 0].squeeze(),
  )
for i in range(len(x_val)):
    image = x_train[i]
    gt_mask = y_train[i]
    image = np.expand_dims(image, axis=0)
    pr_mask = model.predict(image).round()
    visualize(
        image=image.squeeze(),
        gt_mask=gt_mask[..., 0].squeeze(),
        pr_mask=pr_mask[..., 0].squeeze(),
    )

import joblib
import tensorflow as tf
import keras
import json
from keras.models import model_from_json

json_file = open("/content/drive/MyDrive/Modelos/Data/network.json", 'w') 

model_json = model.to_json() 
json_file.write(model_json) 
json_file.close() 
 
model.save_weights("/content/drive/MyDrive/Modelos/Data/network.h5")

json_file = open("/content/drive/MyDrive/Modelos/Data/network.json", 'r') 
loaded_model_json = json_file.read() 
contents = json.loads(loaded_model_json)

json_file.close() 

loaded_model = model_from_json(loaded_model_json) 
  
loaded_model.load_weights("/content/drive/MyDrive/Modelos/Data/network.h5") 

imag_str = "/content/drive/MyDrive/Modelos/Data/test_ima/ima/20161225_TPZ_00440.png"
path_mask =  "/content/drive/MyDrive/Modelos/Data/test_mask/mask/20161225_TPZ_00440_gt_bbox.jpg"
image = resize(imread(imag_str)/255,(256,256)).astype('float32')
mask = resize(imread(path_mask)/255,(256,256)).astype('float32')
image = np.expand_dims(image, axis=0)
pr_mask = loaded_model.predict(image).round()
visualize(
        image=image.squeeze(),
        gt_mask=mask[..., 0].squeeze(),
        pr_mask=pr_mask[..., 0].squeeze(),
  )

#eje = joblib.load(data_dir+'/modelo1.joblib',custom_objects={'f1':f1})
for i in range(len(x_val)):
    image = tmp_xval[i]
    gt_mask = y_val[i]
    image = np.expand_dims(image, axis=0)
    pr_mask = loaded_model.predict(image).round()
    visualize(
        image=image.squeeze(),
        gt_mask=gt_mask[..., 0].squeeze(),
        pr_mask=pr_mask[..., 0].squeeze(),
    )